<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>YOOO</title>
	<link rel="stylesheet" type="text/css" href="build/css/main.css">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
</head>
<body>
	<div id="textWrap">
		<section>
			<h2>
				design is quite hard<br>
if machines learn to do it<br>
we might starve and die<br>
			</h2>
		<h3>Abstract</h3>
			<p>In an age where artificial intelligence is expanding towards more and more areas, is is essential for graphic designers to learn about this topic and understand its potential implications. This thesis attempts to tackle the problem of computational creativity with a focus on graphic design. It examines several existing technologies aiming to find out how they can be used to carry out the tasks that graphic designers perform. By finding out the potential capabilities of these technologies, graphic designers may know how how to adapt to the technological revolution that is rapidly approaching.</p>
			<p>Developing an artificial intelligence capable of autonomous design is a very difficult problem, because graphic design, and especially “good graphic design" is an intrinsically subjective and abstract concept which is very difficult to quantify and fully automate. This thesis aims to find to what extent this kind of autonomous design is possible.</p>
			<p>The focus of this research is mostly based on practical, hands-on experimentation with existing technologies like deep neural networks, generative adversarial networks and genetic algorithms. By conducting several experiments, it might be possible to identify the potential of these technologies, as well as its limitations.</p>
			<p>Followed by these experiments is [will be] a reflection on the potential disruption that machine learning can have on the discipline of graphic design and how designers can confront a latent future of obsolesce.</p>
		</section>
		<section>
			<h3>On genetic algorithms and fitness functions</h3>
			<p>One of the most difficult problems of generating graphic design through artificial intelligence, and more specifically genetic algorithms, is that there is a substantial need for human assistance in the learning process of the algorithm. This is due to the fact that an essential element of the genetic algorithm is the fitness function. The fitness function is a loss function which encompasses all the merits or qualities of a given generated item. In other words, it evaluates with a score, how good it is compared to the other samples generated by the same algorithm. So, while programming a genetic algorithm, one would write the fitness function according to the goal that wants to be reached. For example, if we were generating a race car, our fitness function would probably consist of its speed, and secondarily also its efficiency, durability and/or safety. Subsequently, when the genetic algorithm is running, it would use this fitness function to define which of the generated cars is the best, and it would generate with the best characteristics accordingly.</p>
			<p>When it comes to generating race cars, writing a fitness function is fairly straight forward, as we have a relatively clear idea of what a good race car consists of. But if our goal is to generate a graphic design poster through genetic algorithms, things become much more difficult. This is because, as opposed to race cars, the elements that make good design good, are not clear at all.</p>
			<p>If we have no clear way of steering the generation process of the genetic algorithm towards what we, subjectively think is good, then the algorithm can’t possibly output satisfactory results. Therefore, what is usually done to solve this problem is to use assisted learning methods, such as manually training the model. A well known example of this is the interactive installation titled Galápagos, by <span class="link">Karl Sims</span>, in which twelve computers generated a series of virtual “organisms” that would evolve through time. The goal was to create the most aesthetically interesting organisms, so the fitness of each sample was also defined by a very subjective and arguably unquantifiable factor. A sensor was placed in front of each computer screen, counting for how long each sample was observed by a viewer. This input served as the fitness of each sample, therefore allowing the samples to breed, mutate and evolve based on a subjective characteristic like “being interesting”.</p>
			<p>Assisted learning is very effective because the person training the model is in full control of what what is being learned and therefore also has a very high degree of control over the results of the algorithm, especially with manual training, such as Karl Sim’s Galápagos. Nevertheless, there are important disadvantages to these models. The most significant one is that the speed of training is extremely slow compared to other machine learning models. Models with a built in automatic fitness function will be able to compute and evolve the samples at an incomparable speed, which means that the process not only becomes more convenient, but that the samples are allowed to evolve indefinitely, possibly to their “peak” evolutionary state.  Additionally, genetic <span class="link">algorithms</span> have a huge potential for problem solving, as they may explore unexpected alternatives that the user may not have considered. Manually determining the fitness of each sample not only slows the process down, but it hinders its potential of generating more innovative solutions. It greatly subjects the result to the human preconceptions of what the result is supposed to be.</p>
			<p>A possible solution to the fitness function problem, is to develop a double model, consisting partly of a genetic algorithm as well as a deep neural network (DNN) that would serve as a classifier that would control the fitness function of the genetic algorithm. The DNN can be trained with datasets of graphic design image files (in this case posters, to avoid format issues). These datasets have to be previously classified by hand into “good” and “bad” posters. Through training, the DNN could learn to identify all the variable and subjective elements that make a poster good. If this experiment is successful, the trained DNN would be able to get a poster as an input, evaluate it, and output a score for it.</p>
			<p>It is important to note that since this network will be trained with a dataset pre-classified by me, the process and outcome will be undoubtedly influenced by my subjective interpretation of what quality means in design. Therefore, in a scenario where the trained DNN works, it would reflect, to a certain extent, my own taste and opinions in its decisions. Additionally if this DNN working to gather were to generate posters, it would by no means result in anything that could be considered objectively “good”, as there is no such thing.</p>
			<p>The purpose of the following experiment is to determine whether using a DNN to determine the fitness function of a genetic algorithm is viable. The results are rather uncertain, mostly because the DNN could or could not accurate capture the pattern that links all the good posters together and all the bad posters together. DNNs are very good at finding patterns, which is why they often excel at image recognition and <span class="link">classification</span>. If we were to try to classify dolphin images, it would find all the common characteristics that constitute a dolphin image, and effectively classify any image within this class. The difficulty with graphic design posters, lies in the fact that the posters themselves vary vastly. While a dolphin image will always contain some kind of dolphin, posters will most of the time not have any resemblance to each other. My hypothesis is that the DNN will be able to find certain patterns in the way typography is placed, as well as the use of color throughout the image. However, it is very possible that the trained DNN will have a hard time classifying with perfect accuracy, given the fact that the differences between the good and bad posters is not so evident, nor objective. On the other hand DNNs are known for being able to identify differences between very similar inputs.</p>
			<p>The training data consists of little over 3000 images, half of which are pre-classified as bad and the other as good. The training will be done using Google’s Tensorflow machine learning library, which provides a basic structure for the deep neural network. In order to be able to accurately identify patterns, shapes, colors, and edges, I will be using a pre-trained model called Inception-V3, which already has the ability to recognize over 1000 classes with a high degree of accuracy. Subsequently, I will add the two classes “good poster”, and “bad poster” and train the network for 24 hrs.</p>
			<p>After successful training, the DNN was ready to be to be tested. Any number of posters can be fed to the DNN, an will be classified, on a scale from 0 to 100, on the network’s interpretation of good and bad. I avoided classifying any of the posters that were in the original dataset, as they would probably return an output of 100% good or bad, according to my initial pre-classification. Following are some of the results.</p>
			<p>Given the subjective nature of the experiment, it is difficult to evaluate whether the experiment was successful or not. To test this, before feeding the posters do the DNN, I rated the posters myself; if the rating of the DNN matched mine, it would mean that it works accurately. To measure the accuracy of the classification, I calculated the difference with each of my scores with the DNN scores, resulting on a number between 0 and 100, 0 being complete accuracy. The overall accuracy was 21.9. This indicates that that in general the network can indeed classify the posters but with a low degree of accuracy. In the majority of the tests, the score provided by the DNN somewhat corresponded to my score. But in some case the DNN’s classification was completely off. As I mentioned in my hypothesis, this is likely due to the similarity between the “good” and “bad” training data, and an inability of the DNN to detect these subjective characteristics. In all likelihood, there are even many cases where there is no real patterns linking good posters together.</p>
			<p>I believe that a way to improve this DNN, and its resulting accuracy, would be to train using several sub-classes. Grouping all good and bad posters is way too general which impedes the network to detect patterns accurately. For example, in the “good” data set, there is a modernist poster by Wim Crouwel, as well as an extremely hectic poster by Gilles de Brock. The vast differences between these posters make it very hard for a DNN to find a clear pattern. Therefore, a possible solution would be to make sub-categories and subsequently sort those into the “good” or “bad” classes. However, this would require a lot of manual labour, which is why this DNN was being trained in the first place, which might defeat the original purpose.</p>
			<p>The classifying accuracy of DNN can go up to the 90 - 99% range. The accuracy of this poster classifier is much lower, which means that it’s probably not the best solution for the fitness function problem. Nevertheless, it would be interesting what would be the result of pairing this DNN with a genetic poster generator, because it would allow us to see what the DNN’s subjective interpretation of what a good poster is, giving us insight on how these networks find patterns and learn from graphic design data.</p>
		</section>
	</div>
	<div id="images">
		<img src="img/galapagos-icc240.jpg">
		<img src="img/lel.png">
		<img src="img/nick.png">
	</div>
	<section><p id="measureTxt"></p></section>
	<!-- <div id="controls">
		<input type="range" min="0" max="255" value="255" class="slider" id="r">
		<input type="range" min="0" max="255" value="255" class="slider" id="g">
		<input type="range" min="0" max="255" value="255" class="slider" id="b">
	</div> -->
	<script src="build/js/main.js"></script>
</body>
</html>