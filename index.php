<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="build/css/main.css">
	<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script> -->
	<script src="build/js/jquery-1.10.2.min.js"></script>
	<title>Please be patient, I’m still learning.</title>
</head>
<body>
	<div id="prevDesigns">
		Filter by rating:  
		<span id="filter0" class="filter">0</span>  
		<span id="filter1" class="filter">1</span>  
		<span id="filter2" class="filter">2</span>  
		<span id="filter3" class="filter">3</span>  
		<span id="filter4" class="filter">4</span>  
		<span id="filter5" class="filter">5</span>  
		<span id="filter6" class="filter">6</span>  
		<span id="filter7" class="filter">7</span>  
		<span id="filter8" class="filter">8</span>  
		<span id="filter9" class="filter">9</span>  
		<span id="filter10" class="filter">10</span><br><br>
		<span id="designIds"></span><br><br>
		<span class="random">Random</span>
		

	</div>
	<div id="landingMessage">
		This website's design is driven by a genetic algorithm. The design that you will see is generated by a process of evolution based on other user's ratings of previous designs. Please rate the current design by clicking the green bar to further evolve future designs. Thank you.<br><a href="">[close]</a>
	</div>
	<div id="bg"></div>
	<header class="bottom">
		<span id="headerTitle">
			Design #<span id="designCounter">1</span>&nbsp;&nbsp;&nbsp;&nbsp;
			Generation #<span id="generationCounter">1</span>&nbsp;&nbsp;&nbsp;&nbsp;
			Specimen #<span id="specimenCounter">1</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		</span>
		<span id="genInfo">
			
			<span id="prevDesignsBtn" href="#">View previous designs</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			<span id="redesign" href="#">Redesign</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
			Rate this design:&nbsp;<div id="rateBar"><div id="rateBarInner"></div></div>&nbsp;
		</span>
	</header>
	<div id="mainWrap">
		<div id="textWrap">
			<section id="landing">
				<h2>
					Please be patient,<br>
					I’m still learning.
				</h2><br>
				<h4>Use cases and implications of machine learning in graphic design</h4>
			</section>
			<section id="toc"><h3>Contents</h3><br><br></section>

			<section>
				<h3>Abstract</h3>
				<p>As artificial intelligence rapidly expands towards new areas, it is essential for graphic designers to learn about this topic and understand its potential implications. It is still very unclear what these emerging technologies might be able to achieve in a creative field like graphic design, given that it is very difficult to emulate a form of human creativity that is able to transform, combine and explore a vast conceptual space, like graphic designers do. This thesis aims to envision what might be possible through machine learning in the future, and specifically through which methods this might be achieved.</p>
				<p>Developing an artificial intelligence capable of autonomous design is a very difficult problem, because graphic design, and especially “good graphic design" is an intrinsically subjective and abstract concept which is very difficult to quantify and fully automate. This thesis aims to examine to what extent this kind of autonomous design is possible, how it might be achieved, and what this achievement would imply.</p>
				<p>This research is mostly carried out through experimentation with several existing technologies like deep neural networks, generative adversarial networks and genetic algorithms, evaluating their capabilities as well as their limitations to perform the tasks of a graphic designer. The use of these technologies in the area of graphic design remains virtually unexplored, which is why a deeper understanding of their capabilities could point out how we could adapt to the major changes that are likely to occur to the discipline of graphic design in the near future.</p>
			</section>
				
			<section>
				<h3>Introduction</h3>
				<p>Machine learning is a field in computer science which revolves around providing a computer program the capacity to learn something without direct programming. For example, one might try to get a computer to excel at <span class="imgLink">chess</span><span class="linkedImg sideContent"><img src="img/chess.jpg" /><br><span class="imgCaption">Garry Kasparov playing against Deep Blue, the chess-playing computer built by IBM.</span></span>. One way to do that would be to program all the rules, as well as what move it’s supposed to perform given a certain configuration of the board. Another possibility would be to have it calculate all the potential moves in the future and to chose from the best solution. Both of these options would be far from ideal because it would take a very long time and a lot of effort to program them manually. Furthermore, programming the computer’s moves manually would mean that all kinds of human errors would be encoded into the program, restraining it from the potential of coming up with a solution the programmer didn’t think of.</p>
				<p>An approach based on machine learning, would likely be much more efficient, effective and ultimately produce a better chess player. A machine learning model would provide the program the ability to learn the game by itself, gradually improving until reaching its goal, all with very little human intervention. There are several different kinds of models of machine learning, several of which are inspired by natural, biological processes. Some well known examples are genetic algorithms, inspired by Darwinian evolution and neural networks, largely inspired by how groups of neurons work inside the human brain. Both of these, as well as other machine learning models, will be discussed in detail below.</p>
				<p>In the past couple years, we have begun to see this area of computer science really thriving and achieving very impressive feats.<sup class="refLink">1</sup><span class="linkedRef sideContent">1. S. Venkatachalam, Founding Partner, Centerview Capital Technology. (2017, May 24). 2017 is the year of artificial intelligence. Here's why. Retrieved December 10, 2017, from <a href="https://www.weforum.org/agenda/2017/05/2017-is-the-year-of-artificial-intelligence-here-s-why/">weforum.org</a></span> This has been accomplished with methods developed over a decade ago, but that only now we can carry out because of an exponential increase in the computing power available to us, and the equally vast amount of data at our disposal.</p>
				<p>Machine learning methods can now accomplish tasks we couldn’t have deemed possible 5 years ago. In fact, without the general population’s knowledge, it is an essential component to many of the technologies we use daily, it picks the songs we listen to, the results of out internet searches, it translates our texts, verifies our identities, predicts the weather and so on. Furthermore, the range of tasks AI performs in our daily life will most probably dramatically increase in the coming years. There is no doubt that these systems can accomplish a wide range of tasks more efficiently than humans, and in many cases with much better results.</p>
				<p>Technological progress is by no means linear.  As humans create tools, they can make use of that new tool to create a better one faster than what would have been possible without the initial tool. For example, after the computer was invented, inevitably computers became a tool for designing better computers much more efficiently. This better computer will subsequently allow for even better computers to be produced even faster, and so on. Because of this phenomenon, the speed in which technological innovation occurs is exponential. Many people fear this fact, especially with artificial intelligence in the picture, foreseeing a scenario where an artificial intelligence would be able design a superior version of itself, repeating this process several times and at an increasing speed, reaching some kind of super intelligence, which is regarded as a great threat by many.  Whether this scenario is plausible or not, it is a fact that artificial intelligence and machine learning systems will be far, far superior in the near future.</p>
				<p>The fact that technological growth is rapidly accelerating is very relevant: while this text examines the potential of contemporary machine learning systems in the field of graphic design, the limitations that now constrain a machine’s ability to learn design will be greatly reduced in the future. This thesis apart from analyzing current machine learning models and their current capability to design, attempts to make a proposition on how these systems might be improved, allowing for new, more powerful ways to achieve this goal.</p>
			</section>

			<section>
				<h3>On genetic algorithms and fitness functions</h3>
				<p>Genetic algorithms are a model for machine learning inspired by biological evolution. As described by Darwin, life on Earth has undergone a very long process of selection, or as it is often called, survival of the fittest. The way that this works is that the fittest specimens (individuals most suited for survival) will live longer and therefore be able to mate more often than other less fit individuals. Take primates as an example, they developed superior intelligence over millions of years, the individuals that randomly acquired higher intelligence through mutations, had a greater chance of survival and passed on their genes to the next generations, very slowly optimizing the species for survival.</p>
				<p>The same process is emulated within a genetic algorithm. There are several variants of this model, but the main premise is that a goal, also known as the fitness, is defined. Afterwards a group of individuals, known as a generation, is created with random characteristics. Each individual of this first generation is measured based on its fitness, and reproduced accordingly, with the fittest individuals reproducing more often. During reproduction the characteristics of the individuals will mix and mutate randomly, allowing future generations to evolve further. The main feature of genetic algorithms, as well as biological evolution, is optimization. The subjects evolve always towards a version of themselves that is more optimized to meet their goal. However, it is important to notice that since optimization is based on random mutation, the subjects will probably never reach the absolute best possible version of themselves, but rather an approximation.</p>
				<p>Genetic algorithms allow us to solve problems very efficiently. Instead of searching for all possible solutions, this model is selective and explores what it deems more valuable. For this reason it commonly used to carry out several tasks, solving mathematical problems, developing medicines and designing products. Since this processes occur with very limited human intervention, the results are often very unexpected and seem counterintuitive, but nonetheless, perform much better than their human-designed predecessors. A good example of this is NASA’s spacecraft <span class="imgLink">antenna</span><span class="linkedImg sideContent"><img src="img/antenna.jpg" /><br><span class="imgCaption">The 2006 NASA ST5 spacecraft antenna.</span></span> which, despite its alien appearance, is remarkably optimized to carry out its task. These strange design choices are one of the reasons why it would be very interesting to explore the idea of genetic algorithms producing something visual like graphic design.</p>
				<p>Unlike designing antennas or any other thing with a specific goal, generating graphic design would rely heavily on human assistance during the learning process of the algorithm. This is due to the fact that an essential element of the genetic algorithm is the fitness function. The fitness function encompasses all the merits or qualities of a given generated item. In other words, it evaluates with a score, how good it is compared to the other samples generated by the same algorithm. So, while programming a genetic algorithm, one would write the fitness function according to the goal that wants to be reached. For example, if we were generating a race car, our fitness function would probably consist of its speed, and secondarily also its efficiency, durability and/or safety. Subsequently, when the genetic algorithm is running, it would use this fitness function to define which of the generated cars is the best, and it would reproduce a new generation, passing on this car’s characteristics, combining them with good qualities of other cars, aiming to generate a superior car eventually.</p>
				<p>When it comes to generating race cars, writing a fitness function is fairly straight forward, as we have a relatively clear idea of what a good race car consists of. But if our goal is to generate a graphic design poster with a genetic algorithm, things become much more difficult. This is because, as opposed to race cars, the elements that make good design good, are not clear at all.</p>
				<p>If we have no clear way of steering the generation process of the genetic algorithm towards what we subjectively think is good, then the algorithm can’t possibly output satisfactory results. Therefore, what is usually done to solve this problem is to use assisted learning methods, such as manually training the model. A well known example of this is the interactive installation titled <span class="imgLink">Galápagos</span><span class="linkedImg sideContent"><img src="img/sims2.gif" /><br><img src="img/sims1.jpg" /><br><span class="imgCaption">Karl Sim’s Galapagos. This image depicts "relatives" from the same evolutionary progression.</span></span>, by Karl Sims, in which twelve computers generated a series of virtual 3D “organisms” that would evolve through time. The goal was to create the most “aesthetically interesting” organisms, so the fitness of each sample was also defined by a very subjective and arguably unquantifiable factor. A sensor was placed in front of each computer screen, counting for how long each sample was observed by a viewer. This input served as the fitness of each sample, allowing them to breed, mutate and evolve based on a subjective characteristic like “being interesting”.</p>
				<p>Assisted learning is effective in the sense that the person training the model is in full control of what what is being learned and therefore over the results of the algorithm, especially with manual training, such as Karl Sim’s Galápagos. Nevertheless, there are important disadvantages to these models. The most significant one is that the speed of training is extremely slow compared to other machine learning models. Genetic algorithms running in an unassisted manner, with a built in fitness function will be able to compute and evolve the samples at an incomparable speed, which means that the process not only becomes more convenient, but that the samples are allowed to evolve indefinitely, possibly to their “peak” evolutionary state. Additionally, genetic algorithms have a huge potential for problem solving, as they may explore unexpected alternatives that the user may not have considered. Manually determining the fitness of each sample not only slows the process down, but it hinders its potential of generating more innovative solutions. It greatly subjects the result to the human preconceptions of what the result is supposed to be.</p>
				<br>
				<p>Finding a way to automate the fitness function of a genetic algorithm would allow us to avoid the issues of assisted training models mentioned above. A possible solution to this, would be to develop a double model, consisting partly of a genetic algorithm as well as a deep neural network (DNN). Deep neural networks are complex machine learning models designed to interpret data. When a DNN is fed training data, it passes it through several layers, and therefore is able to analyze several different aspects of it. From the findings it makes on each sample of the training data, it can find patterns in the whole dataset. Due to the fact that DNNs are very good at pattern recognition, they are often used for image classification. The task that humans would perform to manually train a genetic algorithm that generates graphic design is essentially classification between good and bad posters. Therefore, the DNN could possibly replace the human judgement process that would normally hinder the potential outcome and efficiency of the genetic algorithm.</p>
				<p>The purpose of the following experiment is to determine whether using a DNN to determine the fitness function of a genetic algorithm is viable solution to automate the learning process of a genetic algorithm. The results are rather uncertain, mostly because the DNN could or could not accurate capture the pattern that links all the good posters together and all the bad posters together. DNNs are very good at finding patterns, which is why they often excel at image recognition and classification. If we were to try to classify dolphin images, it would find all the common characteristics that constitute a dolphin image, and effectively classify any image within this category. The difficulty with graphic design posters, lies in the fact that the posters themselves vary vastly. While a dolphin image will always contain some kind of dolphin, posters will most of the time not have any resemblance to each other. My hypothesis is that the DNN will be able to find certain patterns in the way typography is placed, as well as the use of colour throughout the image. However, it is very possible that the trained DNN will have a hard time classifying with perfect accuracy, given the fact that the differences between the good and bad posters is not so evident, nor objective. On the other hand DNNs are known for being able to identify differences between very different inputs.</p>
				<p>Even though avoiding an unbiased result is unachievable, it is possible to reach a higher degree of impartiality. I attempted to gather posters generally regarded as “good”, by gathering data from poster competitions, museums and other forms of archives. This way the results will be slightly less based on my own personal opinion and backed by a broader consensus.</p>
				<p>However, it is important to note that even if there is some consensus backing the training data, it is ultimately chosen by me. I made the decision to chose certain poster competitions or museums that I believed contained “valuable” content. For this reason the process and outcome will be inevitably influenced by my subjective interpretation of what quality means in design. Therefore, in a scenario where the trained DNN works, it would reflect, to a certain extent, my own taste and opinions in its decisions.</p>
				<p>The DNN can be trained with large datasets of graphic design image files. In this case the <span class="imgLink">training data</span><span class="linkedImg sideContent"><img src="img/tposters.png" /><span class="imgCaption"><br>A small portion of the “good poster” training data</span></span> consists of little over 3000 images, half of which are pre-classified as bad and the other as good. Through training, the DNN should learn to identify all the variable and subjective elements that make a poster good. If this experiment is successful, the trained DNN would be able to get a poster as an input, evaluate it, and output a score for it. The training will be done using Google’s Tensorflow machine learning library, which provides a basic structure for the deep neural network. In order to be able to accurately identify patterns, shapes, colours, and edges, I will be using a pre-trained model called Inception-V3, which already has the ability to analyze images with a high degree of accuracy, detecting a very varied range of characteristics and details in the images. Subsequently, I will add the two categories “good poster”, and “bad poster” and train the network for 24 hrs.</p>
				<p>After successful training, the DNN was ready to be to be tested. Any number of posters can be fed to the DNN, an will be classified, on a scale from 0 to 100, on the network’s interpretation of good and bad. I avoided classifying any of the posters that were in the original dataset, as they would return an output of 100% good or bad, according to my initial pre-classification. Following are some of the results. </p>
				<span class="imgLink"></span><span class="linkedImg sideContent"><img src="img/goodposters.png" /><span class="imgCaption"><br>Posters rated as “good”, with scores of 89%, 88%, 74%, 62% and 55% good.</span><br><br><img src="img/badposters.png" /><br>Posters rated as “bad”, with scores of 82%, 77%, 73%, 64%, 63% and 57% bad.</span><p>The quality of DNN image classifiers is usually measured by its accuracy. It is presented with a number of images to determine how often it is correct. Given the subjective nature of this experiment, it is difficult to evaluate whether the experiment was successful or not. To measure this, before feeding the posters do the DNN, I rated the posters myself; if the rating of the DNN matched mine, it would mean that it works accurately. Clearly, it is extremely unlikely that the ratings provided by a computer program would correspond mine exactly. But if the classification is accurate, there should be a general trend in which both the algorithm and I rate the poster with a high or low score consistently. To measure the accuracy of the classification, I calculated the difference with each of my scores with the DNN scores, resulting on a percentage, 100% meaning perfect accuracy. </p>
				<p>The overall accuracy was 78.1%. This indicates that that in general the network can indeed classify the posters with a decent degree of accuracy. In the majority of the tests, the score provided by the DNN somewhat corresponded with my score. However there were a couple cases where the DNN’s classification was completely off. This means that the rating provided by the DNN are rather unpredictable. As I mentioned in my hypothesis, this is likely due to the similarity between the “good” and “bad” training data, and an inability of the DNN to detect these subjective characteristics. In all likelihood, there are even many cases where there are no actual patterns linking good posters together.</p>
				<p>I believe that a way to improve this DNN, and its resulting accuracy, would be to train using several sub-classes. Grouping all good and bad posters is way too general which impedes the network to detect patterns accurately. For example, in the “good” data set, there is a modernist poster by Wim Crouwel, as well as an extremely hectic poster by Gilles de Brock. The vast differences between these posters make it very hard for a DNN to find a clear pattern. Therefore, a possible solution would be to make sub-categories and subsequently sort those into the “good” or “bad” categories. However, this would require a lot of manual labour, which is why this DNN was being trained in the first place, which would defeat the original purpose.</p>
				<p>Even though the overall accuracy of the classifier was reasonably good, the fact that there were a high number of outliers, means that it’s probably not the best solution to automate the fitness function of a graphic design generator. Nevertheless, it would be interesting what would be the result of pairing this DNN with a genetic poster generator, because it would allow us to see what the DNN’s subjective interpretation of what a good poster is, giving us insight on how these networks find patterns and learn from graphic design data.</p>
			</section>

			<section>
				<h3>On generative adversarial networks</h3>
				<p>Deep neural networks are very good at making sense of data, which is why their most common use is not to generate content but to analyze it. However this is not their only application: <span class="imgLink">deep dream</span><span class="linkedImg sideContent"><img src="img/deep-dream.jpg" /><span class="imgCaption"><br>Deep dream image creates dogs from pizza.</span></span> images are one example of visuals generated by a DNN, and the process generating them is very interesting. When a DNN is analyzing visual data, it generates a model of the thing it is analyzing. For example, if we were training a DNN to detect dogs in images, it would generate a very complex model of what a dog looks like. Normally, this trained model would be used to classify or detect dogs. But deep dream images reverse this process, so it is fed an image that does not contain a dog, and it is tasked with finding dog-like patterns in the image, and accentuating those patterns. This would result in an image fully covered in dogs, where there were previously none.</p>
				<p>Even though deep dream images can be fascinating, they tend to output very similar results. They have a consistent stylistic characteristic that seems to be intrinsic to the technique. According to Kyle E. Jennings, an important characteristic of a creative artificial intelligence is autonomous change, which refers to the model’s ability to evolve or break it’s own boundaries. A different concept of creativity, proposed by Margaret A. Boden, is much broader, suggesting that creativity consists mainly of novelty and value. Even though deep dream images could be considered a valuable novelty, it is arguable that the deep dream image “genre” as a whole is what is novel, and not the new content generated using the technique. In other words, deep dream images are so homogenous that probably newer iterations of them are likely not be very valuable or novel.</p>
				<p>However there is another model that might way more versatile and powerful in generating images. Computer scientist Ian Goodfellow developed the concept of generative adversarial networks, or GANs, in 2014. This machine learning model is somewhat similar to the model I proposed in the previous chapter, because it consists of two independent AIs, one which generates images, and another that evaluates them. The model I proposed worked with the pairing of a genetic algorithm and a DNN, while GANs operate with two DNNs. The first DNN is called the generator and its role is to look at the training data and try to generate a similar image. The second DNN is called the discriminator, and it is sometimes presented with the images created by the generator and sometimes with images from the training data. It’s role is to try to determine whether the image it is presented with is a “real image” (from the training data) or a “fake” one (generated); if the discriminator is fooled, it means the generated image is convincing. As the GAN is trained, both the discriminator and the generator become smarter, and therefore the generator is obliged to improve, in order to keep on “fooling” an increasingly intelligent discriminator. As the name suggests, they work as adversaries: constantly trying to outsmart and beat each other.</p>
				<p>An analogy that is commonly used to explain GANs is of a criminal producing counterfeit money bills (as the generator) and a policeman (as the discriminator). The criminal will do its best to create money bills that seem real, and if he succeeds, the policeman will have to find new ways to detect the counterfeit bills. Once the policeman finds a way to detect the fake bills, the criminal will have to improve, making better, more realistic bills. This process would go on and the criminal, policemen and counterfeit money would necessarily improve over time. Ideally, it would eventually reach a point where the counterfeit money is completely indistinguishable from real money. GANs use this concept with their dual DNN model which allows them to generate a wide variety of complex images very accurately.</p>
				<p>Unlike stand-alone DNNs and deep dream images, GANs have a higher potential of generating more diverse results. Of course, this depends on giving an appropriate task and good training data, but assuming this is done correctly, GANs are capable of generating images free of stylistic restraints. This increases the probability that an image generated by a GAN could be more valuable or novel than deep dream image. GANs are very often trained with photographic material, because </p>
				<p>In order to explore the potential as well as the limitations of GANs, I devised two  experiments that would allow me to evaluate how this technology could be used to either generate graphic design or isolated graphic design elements. The first experiment consists of trying to generate a poster. The training data will be very similar to that of the “fitness function” experiment in the previous chapter, consisting of around 3000 poster images. It will be carried out in an unassisted manner, so the posters have not ben pre-classified in any way. This means that there is a wide variety of posters of many different styles and authors. What I intend to find out is if the GAN will be able to find a pattern in all these posters and represent it in the generated posters.</p>
				<p>Generative adversarial networks have gained considerable attention lately for the achievement of generating very realistic images of <span class="imgLink">human faces</span><span class="linkedImg sideContent"><img src="img/faces.png" /><span class="imgCaption"><br>Faces generated by a GAN.</span></span>. It was trained with a huge database of celebrity faces and it outputs uncanny, yet photorealistic faces of people that do not exist. This project was successful because the GAN was able to grasp what a human face consists of. This is challenging if the goal is to generate a poster, because posters vary considerably, while faces all have characteristics in common that are positioned similarly in each face. My hypothesis is that, with enough training, the GAN will be able to find typography as the recurring pattern in the posters, although it might generate it in a rather erratic way, given that this model is not programmed to coherently incorporate words, and much less so words with meaning. Apart from finding typography as a pattern, if the experiment is very successful, the GAN might be able to detect and reproduce certain geometric patterns that are very often used accompanying the typography. It is also a possibility that the GAN will fail, in which case the result will probably consist of very abstract, pattern-less shapes.</p>
				<p>For this experiment I will be using a GAN called HyperGAN. Unlike the previous experiment, I will not be using a retrained model, which means that HyperGAN will not be able to recognize anything at all, which is convenient because DNNs and GANs are often used with photographic material which is very different to graphic design images. This will likely allow the result to be cleaner and flatter, as is usually graphic design compared to photographs. On the other hand training will take much more time and computational power.</p>
				<p>After training for around 48 hours, the process flatlined, meaning the results stopped improving. Overall, the experiment was unsuccessful as the generated images did not resemble any of the posters provided as training data. The results were very abstract which leads me to believe that the training data was so varied, that the GAN had difficulties finding any strong patterns. Nevertheless, the results were interesting for two main reasons. Even though the GAN was not able to find a pattern strong enough to generate a poster, there is a reoccurring element in most of the 32 generated posters, which is elements positioned in a grid-like manner. The elements never seemed to be placed randomly, but rather in order, one next or below the other. This indicates that the GAN found grids this to be the common characteristic in the posters. Another interesting finding, contradicting my hypothesis, was that the use of colour of the GAN was consistently harmonious. As the model is training, it is trying out several colours, but what caught my attention is that even though the colour combinations were changing constantly, it always seemed to output balanced combinations, even generating pairs that I would not have expected to work together.</p>
				<p>Given that the GAN had difficulties finding patterns with such varied training data, I believe an appropriate step is to experiment with more uniform training data such as typography. Furthermore, it might be easier to successfully generate a poster by generating it in isolated steps. For example, generating a background, then the typography, and then some kind of image and later combining them, could be a much more straightforward task for an AI than to generate everything as an individual image. This process would resemble the human process much more, as designers recognize separate elements and work accordingly.</p>
				<p>Therefore, the next experiment consists of attempting to teach HyperGAN what letters are, in theory allowing it to design letters or typefaces. Even though typography is much more consistent visually than posters as a whole, typefaces vary greatly, which is a challenge for the same reasons that the previous HyperGAN experiment failed. Nevertheless, the large diversity within typefaces also allows the GAN to learn from very different designs, enabling it to generate more interesting results.</p>
				<p>In her definition of creativity, Margaret Boden describes three sub-categories of the concept: combinational, transformational and exploratory creativity. What I intend to achieve with this experiment, is for HyperGAN to perform combinational creativity. By training the model with very varied samples of letters, it should be able to generate a new letter, with characteristics from several other letters, as long as the result fits into the GAN’s conception of what that specific letter looks like. Combination is a powerful method, especially with the wide selection of design elements and details that letters display, the possible combinations and results that might be unique and valuable are many.</p>
				<p>I collected around 2000 typefaces, including serifs, sans-serifs, monospaced, scripts, display, slab-serifs, etc. Through a python script, I was able to generate an image for each glyph in each font, resulting in more than 100,000 individual images that served as training data for the GAN. My first attempt to generate letters was with a semi-supervised method, meaning that the training data was pre-classified in folders for each glyph. During the training, HyperGAN’s generator would attempt to generate any letter, and the discriminator would evaluate it, but also classify it as any of 52 different characters (the whole alphabet, uppercase and lowercase). This process would determine which letter is being generated.</p>
				<p>The training took around 18 hours until it flatlined, much less than when it trained to generate more complex and colorful posters. The results were quite varied, as certain letters were generated very clearly and with details, while others never seemed to achieve this level of detail, displaying blurry, blobby forms. I was pleased to see the high degree of variety displayed in the results, as several kinds and weights of letters were generated. Interestingly, the GAN tended to generate the least complex letters. This is because in the early stages of training it would generate rather simple shapes which were immediately classified as the most simple letters, such as Is and Os</p>
				<p>To avoid the tendency to generate the least complex letters (O, Q, I, etc.) it is necessary to train the model with each character individually. Therefore, I repeated the experiment following the same process as before, but only with the letter E. The results were much clearer, probably because the system could work much more efficiently focusing on the elements of one glyph as opposed to 26. Additionally, the GAN would not morph the letters during the process like in the previous experiment, where it could have at some point mistaken the E for an F, for example, hindering the learning process for the E character.</p>
				<p>There is also a wide variety of designs, displaying different shapes, weights and details. It is interesting to see some sort of hybridization within different styles of type design. Clearly, it generated both serif and san-serif typefaces, but in some cases (number 17, for example) the main structure is of a sans-serif but with serifs in the upper half of the letter.</p>
				<p>It is also important to mention that in this case I decided to generate 48 different glyphs, but this model is capable of generating a virtually infinite amount of samples. Clearly this does not mean that it could generate extremely innovative designs each time, but given that the training data contained a very wide variety of typefaces, the model has the potential of generating a huge amount of possibly interesting results.</p>
				<p>This model, once trained to generate a wider range of glyphs, could possibly be used as a tool for type designers, looking for inspiration o new kinds of combinations of details. With moderate human intervention and redefining, these samples can be vectorized and turned in to an actual typeface. However, it is definitely foreseeable that in the near future there could be a similar model that not only learns lettershapes, but spacing, kerning and other parameters. Such a system could automatically generate thousands of typefaces with no human intervention. This is definitely not far fetched, given that all the aspects that conform a typeface are defined by data that can be analyzed and applied by a DNN. So the DNN could be fed with spacing data and learn the most appropriate spacing for the specific kind of letters it generated. Additionally, if a system like this is developed it would only would need to be trained once, after which it would have the ability to generate a vast amount of typefaces very quickly.</p>
			</section>

			<section>
				<h3>Proposing a new model</h3>
				<p>Exploring the capabilities of generative adversarial networks and a combination of genetic algorithms and deep neural networks, has allowed me to identify one of the major problems that prevent the current models from generating design more autonomously and effectively. I noticed a very big disparity between how designers produce their work and how machine learning models attempt to emulate this. This disparity consists of the fact that designers mostly work in steps, in a layered fashion, while the models that I experimented with attempt to generate a poster in one go, as a single image and not as a collection of separate elements. In other words, the machine learning model sees finished products as training and tries to generate them as such.</p>
				<p>When Ian Goodfellow, the creator of the generative adversarial network model, was training a GAN to generate cats, he provided thousands of training images, unaware that some of them were internet memes containing white text as well as the cat photograph. This resulted in the GAN interpreting text as a part of what constitutes a cat image and therefore attempted to generate the text. The text didn’t carry any meaning, of course, and consisted of illegible type-like characters. This is a great example of how this kind of model works in an un-layered way, not distinguishing between different elements. As Ian Goodfellow put it, “The GAN doesn't know what text is so it has made up new text-like imagery”. This makes it unsuitable to generate graphic design, at least in its entirety, as a finished work, because clear and distinguishable elements and legible typography are essential.</p>
				<p>After training for a long time GANs are capable of differentiating different elements within the images. Returning to the example of GANs trained to generate faces of celebrities: in many cases they have sunglasses, and the GAN might be able to identify this characteristic as an individual element. Interestingly, this can be done through simple arithmetic. Once the gan is trained, one can say “man with sunglasses” minus “man” equals “sunglasses” and later add “sunglasses” to “woman”, resulting in an actual image of a woman wearing sunglasses. It would be very interesting to experiment with these techniques on a graphic design generator, even though achieving this would require huge amounts of training data and computational power.</p>
				<p>Therefore, a model that would design in a layered fashion would be much more effective and efficient. It could have separate functions to generate typography, background and foreground imagery individually, for example. This would inevitably result in a clearer poster, as the model would be able to differentiate between different elements and therefore wouldn’t attempt to generate them at once, which results in blobby undefined results, as we saw in the posters design attempts with HyperGAN earlier. Such a model would also be able to operate with vector graphics, instead of outputting a single raster image. This would improve the quality and usability of the results dramatically as well as allowing the user to edit them easily afterwards.</p>
				<p>Another huge advantage of a model like this would be the fact that it could be used as a tool with much more ease. Working with vector graphics would allow for the user of the tool to give certain initial specifications to the model and have it design based on that. For example, one could specify the main text of the poster, as well as secondary text and a photograph, and it could design using those elements.</p>
				<p>To achieve this, DNNs and GANs are not a viable option, especially because they rely on massive amounts of data as training. If the goal is to output vector graphics, huge amounts of vector graphic data is required. Therefore, if I wanted to generate a poster, I would need thousands of quality vector files. This would be extremely difficult to find, as designers regularly don’t publish their work in vector format, but as finalized raster images. For this reason, I believe a genetic algorithm would be the most effective. As we saw before, using a DNN to calculate the fitness of the samples is not viable. Similarly, training the model manually would not be ideal, as rating each design manually would be extremely slow.</p>
				<p>The fact that manually classifying data is very slow, is actually a very common problem. Some solutions have been devised to tackle this problem, mostly relying in having people carryout these tasks in the internet, dividing the daunting amount of work between as many people as possible. In fact, most internet users will have participated in this kind of machine learning training without knowing it. Website log-in pages often contain a captcha asking you to type a house number from a photo, or to “select all images with an orange”. By classifying these images, we provide Google with huge amounts of data to train image classifier DNNs very effectively. Whether you think it’s ethical or not to task your users to unknowingly classify data for you, this demonstrates the power of crowdsourcing huge tasks over the internet.</p>
				<p>Some form of crowdsourced training would be a good solution, greatly accelerating the process. It would also be interesting from a design perspective, allowing users to rate the designs, resulting in a form of collective and democratic process of defining what good design means. Imagine if a popular site like Facebook would allow you to rate its web design, using that data to train a genetic algorithm that would redesign accordingly. It would be very interesting to see the results of this kind of decentralized, collective design practice.</p>
				<p>The model I’m proposing could be successfully programmed in several different platforms, as long as they allow for vector typography and geometry to be used. Probably the best implementation would in the form of a website, making use of a language like Javascript, because then it would be possible for users to rate the samples online, allowing the algorithm to learn and grow fast.</p>
				<p>Apart from specifying the machine learning model it would use and how the fitness/rating would take place, it is important to describe the method this system would utilize to generate graphic design. Assuming it would be programmed using Javascript, the elements would be generated with HTML and styled with CSS, and more complex drawings could be generated through HTML canvas. Both of these elements could be used simultaneously, composing together the finished work. The way in which variations can be made is to gather all the CSS properties and to make them variables, and part of the “DNA” that defines how the poster looks. In a genetic algorithm, the DNA is the data that is passed on to future generations, possibly mutated. In this case, using CSS properties as the DNA of the samples, would allow the algorithm to evolve every detail of the typography and placement of elements. There are more than 200 different CSS properties, meaning that the potential variety of the results is vast. Similarly, the characteristics of the images generated in HTML canvas could be encoded into the DNA would enable the model to generate the kind of complex imagery often found in posters.</p>
				<p>With a generative model like this, there is an essential and difficult aspect that has to be determined by the programmer. One of the most important goals of this generative model is to be able to generate a wide variety of results and to not be confined stylistically. Variety is essential, as it makes the model useful in a border range of situations and it increases the chance that it will produce interesting results. Nevertheless, it is not as simple as it sounds, because giving the algorithm more freedom through more parameters to play around with, dramatically increases the chances that the results will be unsatisfactory. This is not necessarily bad, but it means that it would need much more training to approach passable results. Essentially, this comes down to how much control you hand over to the algorithm, and how much of it you keep as a programmer, by encoding certain rules and limiting its possibilities. Therefore, it is very important to achieve a balance of control human control vs. computer freedom, in order to have a reasonably trainable model while still retaining its ability to produce diverse and novel results.</p>
			</section>

			<section>
				<h3>Machine learning as a tool and designers as curators</h3>
				<p>With the development of machine learning, as well as in increasing interest of graphic designers in technology and programming, it seems very likely that we will be using these technologies as tools in the coming years. Perhaps not using fully autonomous creative machines that will do all the work for us, but as an instrument to expand our capabilities and imagination. Just like we saw with the letters generated using Hypergan, a designer may easily make use of a machine learning algorithm to generate streams of posters, website designs, or pretty much anything, not as final products, but as suggestions that he or she may later work with.</p>
				<p>This idea is both exciting and worrying. It would definitely speed up the design process by replacing the initial stage of design. It might, as we discussed before, provide us with completely new ideas, or even open our perspective into a new paradigm of design we never considered before. However, the fact that we would be handing out control and part of our creative to machine learning systems, might also imply inhibiting our own creative potential.</p>
				<p>Machine learning programs as design tools are likely to become a reality soon. Yet another, more drastic transformation to the field of graphic design becomes a possibility as these programs become more capable. This idea is not something I foresaw, but rather something that emerged as I reflected on my role as a designer attempting to use machine learning to carry out tasks for me. I noticed myself carrying out the role of a curator much more prominently than a regular designer role. Mostly, selecting the appropriate training data for a DNN or a GAN, drastically affecting the outcome, leaned more towards curatorship than design. Likewise, tasks choosing the appropriate machine learning model are more akin to steering the process of design, as opposed to fully controlling it.</p>
				<p> A future where many designers shift their practice towards curating the content that will be fed to machine learning algorithms is not certain but definitely conceivable. Considering the huge potential of these models, in terms of time saving and as tools for exploring different ideas by using a completely foreign methods, would come as no surprise if methods like these become increasingly popular. Perhaps designers, as curators, will input their main inspirations and generate prints this way. Or they may develop websites, gradually teaching the algorithm by assessing which which of its designs are the best; in a way, curating the output of the algorithm itself.</p>
				<p>An algorithm will always carry a certain bias. If it is tasked with generating graphic design, its results will necessarily be determined by how the program was developed, by who trained and how it was trained. However, the same goes for designers themselves, so in this sense the algorithm works as an extension of its user and/or creator. For this reason, we will likely see more and more graphic designers using machine learning tools, expanding their capabilities and allowing them to push the boundaries of what they are currently capable, and possibly venturing into new grounds of graphic design.</p>
			</section>

			<section>
				<h3>Colophon</h3>
				<p> (under construction)
					Design and development<br>
					<strong>Daniel Hernández Chacón</strong>
				</p>
				<p>Special thanks to</p>
				<p>soon</p>
			</section>

<!-- 			<section>
				<span class="imgLink">image example</span><span class="linkedImg sideContent"><img src="img/galapagos-icc240.jpg" /><span class="imgCaption"><br>Caption blerp, 1923</span></span>
				<sup class="refLink">1</sup><span class="linkedRef">1. Hijazi, Samer, Rishi Kumar, and Chris Rowen. “Using Convolutional Neural Networks for Image Recognition.” Cadence, 2015.</span>
			</section> -->


			<?php
				$generationSpecimens = 40;

				// $servername = "localhost:8889";
				$username = "root";
				$password = "root";
				$dbname = "genetic";
				$dbdna = "DNAdb";
				$dbActiveSp = "activeSp";

				// $servername = "thesisdb.thesis.danielh.info";
				// $username = "onions";
				// $password = "****";
				// $dbname = "genetic";

				// activeSp DNAdb

				$generalCount = 0;
				$gen = 1;
				$sp = 1;

				$ratings = [];




				///////////// Get current gen and sp
				$conn = new mysqli($servername, $username, $password, $dbname);
				// Check connection
				if ($conn->connect_error) {
				    die("Connection failed: " . $conn->connect_error);
				} 

				$sql = "SELECT ID, rating FROM activeSp";
				$result = $conn->query($sql);

				if ($result->num_rows > 0) {
				    // output data of each row
				    while($row = $result->fetch_assoc()) {

				    	$generalCount++;
				    	$sp++;

				    	if ($generalCount % $generationSpecimens === 0) {
				    		$gen++;
				    		$sp = 1;
				    	}

				    	array_push($ratings, $row['rating']);

				    }

				} else {
				    echo "0 results";
				}

				$conn->close();


				/////////////////////Get DNA data from DB

				// Create connection
				$conn = new mysqli($servername, $username, $password, $dbname);
				// Check connection
				if ($conn->connect_error) {
				    die("Connection failed: " . $conn->connect_error);
				}

				$sql = "SELECT ID, a0, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24, a25, a26, a27, a28, a29, a30, a31, a32, a33, a34, b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, b11, b12, b13, b14, b15, b16, b17, b18, b19, b20, b21, b22, b23, b24, b25, b26, b27, b28, b29, b30, b31, b32, b33, b34 FROM DNAdb";
				$result = $conn->query($sql);

				$allDNA = [];
				$allADNA = [];

				// $DNA = [];
				// $aDNA = [];

				$genCount = 0;

				$idPad = intval(1 + $sp - 1); // +sp   first ID from initDNA DB

				if ($result->num_rows > 0) {

					$strA = "a";
					$strB = "b";

				    // output data of each row
				    while($row = $result->fetch_assoc()) {

				    	array_push($allDNA, []);
				    	array_push($allADNA, []);


				    	for ($x = 0; $x <= 34; $x++) {
				    		array_push($allDNA[$genCount], $row[$strA . $x]);
				    		array_push($allADNA[$genCount], $row[$strB . $x]);
					    }

					    $genCount++;
				    	
				    	if ($row["ID"] == $idPad) {  ///////////////here goes the specimen ID that will be displayed

							for ($x = 0; $x <= 34; $x++) {
					        	// array_push($DNA, $row[$strA . $x]);
						    }

						    for ($x = 0; $x <= 34; $x++) {
					    		// array_push($aDNA, $row[$strB . $x]);
						    }
					    }
				    }
				} else {
				    echo "0 results";
				}

				$conn->close();

				?>

			<script type="text/javascript">
				var generationSpecimens = 40;

				var dbGeneralCount = parseInt("<?php echo $generalCount; ?>");
				var dbGen = parseInt("<?php echo $gen; ?>");
				var dbSp = parseInt("<?php echo $sp; ?>");
				var dbRatings = <?php echo json_encode( $ratings ); ?>;

				var allDNA = <?php echo json_encode( $allDNA ); ?>;
				var allADNA = <?php echo json_encode( $allADNA ); ?>;

				var activeDNA = allDNA[dbGeneralCount];
				var activeADNA = allADNA[dbGeneralCount];

				var idPadding = 1 - 1; //where DB id starts ---> db = activeSP

				for (var i = 0; i < allDNA.length; i++) {
					for (var j = 0; j < allDNA[i].length-4; j++) { //turn all into ints (exept last 4 floats)
						let intVal = parseInt(allDNA[i][j]);
						allDNA[i][j] = intVal;
					}
				}
				

				$(document).ready(function(){
					// firstGen(); // init only
					// reproduce(); // for debugging
					pushGenSpToDB(); // on
					// landingMessage(); // off
					showLandingOnce(); // on
				});
			</script>

		</div>
	</div>
	<script src="build/js/main.js"></script>
	<script src="build/js/ajax.js"></script>
	<script src="build/js/firstGen.js"></script>
	<script src="build/js/reproduce.js"></script>
	<script src="build/js/prevDesigns.js"></script>

</body>
</html>